{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-27T18:37:19.215095Z","iopub.execute_input":"2023-11-27T18:37:19.215533Z","iopub.status.idle":"2023-11-27T18:37:19.236986Z","shell.execute_reply.started":"2023-11-27T18:37:19.215500Z","shell.execute_reply":"2023-11-27T18:37:19.235799Z"},"trusted":true},"execution_count":161,"outputs":[{"name":"stdout","text":"/kaggle/input/creditcardfraud/creditcard.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**In this notebook, we have tried to implement linear SVM and apply it on Credit Card Fraud Detection Dataset**","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv') # Reading the file .csv\ndf = pd.DataFrame(data)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-11-27T21:07:11.303431Z","iopub.execute_input":"2023-11-27T21:07:11.303856Z","iopub.status.idle":"2023-11-27T21:07:14.504891Z","shell.execute_reply.started":"2023-11-27T21:07:11.303829Z","shell.execute_reply":"2023-11-27T21:07:14.503753Z"},"trusted":true},"execution_count":236,"outputs":[{"execution_count":236,"output_type":"execute_result","data":{"text/plain":"            Time         V1         V2        V3        V4        V5  \\\n0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n...          ...        ...        ...       ...       ...       ...   \n284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n\n              V6        V7        V8        V9  ...       V21       V22  \\\n0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n...          ...       ...       ...       ...  ...       ...       ...   \n284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n\n             V23       V24       V25       V26       V27       V28  Amount  \\\n0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n...          ...       ...       ...       ...       ...       ...     ...   \n284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n\n        Class  \n0           0  \n1           0  \n2           0  \n3           0  \n4           0  \n...       ...  \n284802      0  \n284803      0  \n284804      0  \n284805      0  \n284806      0  \n\n[284807 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>284802</th>\n      <td>172786.0</td>\n      <td>-11.881118</td>\n      <td>10.071785</td>\n      <td>-9.834783</td>\n      <td>-2.066656</td>\n      <td>-5.364473</td>\n      <td>-2.606837</td>\n      <td>-4.918215</td>\n      <td>7.305334</td>\n      <td>1.914428</td>\n      <td>...</td>\n      <td>0.213454</td>\n      <td>0.111864</td>\n      <td>1.014480</td>\n      <td>-0.509348</td>\n      <td>1.436807</td>\n      <td>0.250034</td>\n      <td>0.943651</td>\n      <td>0.823731</td>\n      <td>0.77</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>284803</th>\n      <td>172787.0</td>\n      <td>-0.732789</td>\n      <td>-0.055080</td>\n      <td>2.035030</td>\n      <td>-0.738589</td>\n      <td>0.868229</td>\n      <td>1.058415</td>\n      <td>0.024330</td>\n      <td>0.294869</td>\n      <td>0.584800</td>\n      <td>...</td>\n      <td>0.214205</td>\n      <td>0.924384</td>\n      <td>0.012463</td>\n      <td>-1.016226</td>\n      <td>-0.606624</td>\n      <td>-0.395255</td>\n      <td>0.068472</td>\n      <td>-0.053527</td>\n      <td>24.79</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>284804</th>\n      <td>172788.0</td>\n      <td>1.919565</td>\n      <td>-0.301254</td>\n      <td>-3.249640</td>\n      <td>-0.557828</td>\n      <td>2.630515</td>\n      <td>3.031260</td>\n      <td>-0.296827</td>\n      <td>0.708417</td>\n      <td>0.432454</td>\n      <td>...</td>\n      <td>0.232045</td>\n      <td>0.578229</td>\n      <td>-0.037501</td>\n      <td>0.640134</td>\n      <td>0.265745</td>\n      <td>-0.087371</td>\n      <td>0.004455</td>\n      <td>-0.026561</td>\n      <td>67.88</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>284805</th>\n      <td>172788.0</td>\n      <td>-0.240440</td>\n      <td>0.530483</td>\n      <td>0.702510</td>\n      <td>0.689799</td>\n      <td>-0.377961</td>\n      <td>0.623708</td>\n      <td>-0.686180</td>\n      <td>0.679145</td>\n      <td>0.392087</td>\n      <td>...</td>\n      <td>0.265245</td>\n      <td>0.800049</td>\n      <td>-0.163298</td>\n      <td>0.123205</td>\n      <td>-0.569159</td>\n      <td>0.546668</td>\n      <td>0.108821</td>\n      <td>0.104533</td>\n      <td>10.00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>284806</th>\n      <td>172792.0</td>\n      <td>-0.533413</td>\n      <td>-0.189733</td>\n      <td>0.703337</td>\n      <td>-0.506271</td>\n      <td>-0.012546</td>\n      <td>-0.649617</td>\n      <td>1.577006</td>\n      <td>-0.414650</td>\n      <td>0.486180</td>\n      <td>...</td>\n      <td>0.261057</td>\n      <td>0.643078</td>\n      <td>0.376777</td>\n      <td>0.008797</td>\n      <td>-0.473649</td>\n      <td>-0.818267</td>\n      <td>-0.002415</td>\n      <td>0.013649</td>\n      <td>217.00</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>284807 rows Ã— 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**In the next step, we have partitioned the data into train and test dataset. The complete dataset has 284807 rows. We have considered training data to be all rows till 100,000th row. Since the data-set here is highly imbalanced, we have employed the technique of under-sampling where we have selected all instances of the rare class and have sampled equal number of instances from the abundant class. In addition, we have made the '0' class as '-1' class.**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\ndf_train = df[:100000] # We cut in two the original dataset\ndf_test_all = df[100000:]\n\ndf_train_1 = df_train[df_train['Class'] == 1] # We seperate the data which are the frauds and the no frauds\ndf_train_0 = df_train[df_train['Class'] == 0]\nprint('Number of Fraud Transaction = ' + str(len(df_train_1)))\n\ndf_sample=df_train_0.sample(len(df_train_1))\n\ndf_train = pd.concat([df_train_1, df_sample], ignore_index=True)\n\nX_train = df_train.drop(['Time', 'Class'],axis=1)\nY_train = df_train['Class']\nX_test = df_test_all.drop(['Time', 'Class'],axis=1)\nY_test = df_test_all['Class']\n\nx_train = X_train.values.tolist()\ny_train = Y_train.values.tolist()\nx_test = X_test.values.tolist()\ny_test = Y_test.values.tolist()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)\n\n\n\nfor i in range(len(y_train)) :\n    if y_train[i] == 0 :\n        y_train[i] = -1\nfor i in range(len(y_test)) :\n    if y_test[i] == 0 :\n        y_test[i] = -1\n\nprint(len(x_train))\nprint(len(y_train))","metadata":{"execution":{"iopub.status.busy":"2023-11-27T21:01:52.631531Z","iopub.execute_input":"2023-11-27T21:01:52.631909Z","iopub.status.idle":"2023-11-27T21:01:54.368467Z","shell.execute_reply.started":"2023-11-27T21:01:52.631882Z","shell.execute_reply":"2023-11-27T21:01:54.367253Z"},"trusted":true},"execution_count":233,"outputs":[{"name":"stdout","text":"Number of Fraud Transaction = 223\n446\n446\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We made the class with ","metadata":{}},{"cell_type":"markdown","source":"**Here, we have applied Hinge loss and have tried to optimize the parameters of the model using Gradient Descent algorithm using pytorch module. We tried different values of C and found the best results for C = 0.1**\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport random\n\nC = 0.1\n\ndim = len(x_train[0])\nprint(dim)\nw = torch.autograd.Variable(torch.rand(dim), requires_grad=True)\nb = torch.autograd.Variable(torch.rand(1),   requires_grad=True)\n\nstep_size = 1e-6\nnum_epochs = 100\nminibatch_size = 20\nfor epoch in range(num_epochs):\n    inds = [i for i in range(len(x_train))]\n    random.shuffle(inds)\n    for i in range(len(inds)):\n        L = C * (max(0, 1 - y_train[inds[i]] * (torch.dot(w, torch.Tensor(x_train[inds[i]])) - b))**2)\n        if L != 0: # if the loss is zero, Pytorch leaves the variables as a float 0.0, so we can't call backward() on it\n            L.backward()\n            w.data -= step_size * w.grad.data # step\n            b.data -= step_size * b.grad.data # step\n            w.grad.data.zero_()\n            b.grad.data.zero_()\nprint(w,b)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T21:01:56.841433Z","iopub.execute_input":"2023-11-27T21:01:56.841848Z","iopub.status.idle":"2023-11-27T21:02:05.428940Z","shell.execute_reply.started":"2023-11-27T21:01:56.841816Z","shell.execute_reply":"2023-11-27T21:02:05.427929Z"},"trusted":true},"execution_count":234,"outputs":[{"name":"stdout","text":"29\ntensor([ 0.3942,  0.5590,  0.8403,  0.2795, -0.0033,  0.6035,  0.3497,  0.2059,\n         0.0085,  0.2262,  1.0080,  0.5480,  0.8229,  0.2300,  0.1621,  0.5341,\n         0.0956,  0.3763,  0.4043,  0.6246,  0.2127,  0.7747,  0.8046,  0.2347,\n         0.0692,  0.3383,  0.2353,  0.2641,  0.0104], requires_grad=True) tensor([0.7903], requires_grad=True)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Then, we tried to assess our algorithm, and the best result we got was around 62%.**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nprint('plane equation:  w=', w.detach().numpy(), 'b =', b.detach().numpy()[0])\ndef accuracy(X, y):\n    correct = 0\n    for i in range(len(y)):\n        y_predicted = int(np.sign((torch.dot(w, torch.Tensor(X[i])) - b).detach().numpy()[0]))\n        if y_predicted == y[i]: correct += 1\n    return float(correct)/len(y)\n\nprint('test accuracy', accuracy(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-11-27T21:16:29.431192Z","iopub.execute_input":"2023-11-27T21:16:29.431595Z","iopub.status.idle":"2023-11-27T21:16:34.783617Z","shell.execute_reply.started":"2023-11-27T21:16:29.431567Z","shell.execute_reply":"2023-11-27T21:16:34.782273Z"},"trusted":true},"execution_count":237,"outputs":[{"name":"stdout","text":"plane equation:  w= [ 0.39423925  0.5590441   0.84026676  0.27949435 -0.00326816  0.6034693\n  0.3497401   0.20586573  0.00854138  0.2261779   1.0080268   0.54796916\n  0.8228543   0.22997501  0.16206545  0.53409576  0.09564326  0.37628815\n  0.40430254  0.624594    0.2127448   0.77473867  0.80459267  0.23470433\n  0.06918436  0.3382503   0.2352629   0.26411623  0.01035725] b = 0.79033166\ntest accuracy 0.6263507334678882\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**We tried ensembling methods, since the data-set is highly imbalanced here. We trained 10 Linear SVM models using equal number of positive and negative classes sampled from the training data.**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\ndf_train = df[:100000] # We cut in two the original dataset\ndf_test_all = df[100000:]\n\ndf_train_1 = df_train[df_train['Class'] == 1] # We seperate the data which are the frauds and the no frauds\ndf_train_0 = df_train[df_train['Class'] == 0]\nprint('Number of Fraud Transaction = ' + str(len(df_train_1)))\n\ndf_sample_1 =df_train_0.sample(2*len(df_train_1))\ndf_sample_2 =df_train_0.sample(2*len(df_train_1))\ndf_sample_3 =df_train_0.sample(2*len(df_train_1))\ndf_sample_4 =df_train_0.sample(2*len(df_train_1))\ndf_sample_5 =df_train_0.sample(2*len(df_train_1))\ndf_sample_6 =df_train_0.sample(2*len(df_train_1))\ndf_sample_7 =df_train_0.sample(2*len(df_train_1))\ndf_sample_8 =df_train_0.sample(2*len(df_train_1))\ndf_sample_9 =df_train_0.sample(2*len(df_train_1))\ndf_sample_10=df_train_0.sample(2*len(df_train_1))\n\ndf_train__1 = pd.concat([df_train_1, df_sample_1], ignore_index=True)\ndf_train__2 = pd.concat([df_train_1, df_sample_2], ignore_index=True)\ndf_train__3 = pd.concat([df_train_1, df_sample_3], ignore_index=True)\ndf_train__4 = pd.concat([df_train_1, df_sample_4], ignore_index=True)\ndf_train__5 = pd.concat([df_train_1, df_sample_5], ignore_index=True)\ndf_train__6 = pd.concat([df_train_1, df_sample_6], ignore_index=True)\ndf_train__7 = pd.concat([df_train_1, df_sample_7], ignore_index=True)\ndf_train__8 = pd.concat([df_train_1, df_sample_8], ignore_index=True)\ndf_train__9 = pd.concat([df_train_1, df_sample_9], ignore_index=True)\ndf_train__10= pd.concat([df_train_1, df_sample_10], ignore_index=True)\n\nX_train__1 = df_train__1.drop(['Time', 'Class'],axis=1)\nY_train__1 = df_train__1['Class']\nX_train__2 = df_train__2.drop(['Time', 'Class'],axis=1)\nY_train__2 = df_train__2['Class']\nX_train__3 = df_train__3.drop(['Time', 'Class'],axis=1)\nY_train__3 = df_train__3['Class']\nX_train__4 = df_train__4.drop(['Time', 'Class'],axis=1)\nY_train__4 = df_train__4['Class']\nX_train__5 = df_train__5.drop(['Time', 'Class'],axis=1)\nY_train__5 = df_train__5['Class']\nX_train__6 = df_train__6.drop(['Time', 'Class'],axis=1)\nY_train__6 = df_train__6['Class']\nX_train__7 = df_train__7.drop(['Time', 'Class'],axis=1)\nY_train__7 = df_train__7['Class']\nX_train__8 = df_train__8.drop(['Time', 'Class'],axis=1)\nY_train__8 = df_train__8['Class']\nX_train__9 = df_train__9.drop(['Time', 'Class'],axis=1)\nY_train__9 = df_train__9['Class']\nX_train__10= df_train__10.drop(['Time', 'Class'],axis=1)\nY_train__10= df_train__10['Class']\n\nX_test = df_test_all.drop(['Time', 'Class'],axis=1)\nY_test = df_test_all['Class']\n\nx_train__1 = X_train__1.values.tolist()\ny_train__1 = Y_train__1.values.tolist()\nx_train__2 = X_train__2.values.tolist()\ny_train__2 = Y_train__2.values.tolist()\nx_train__3 = X_train__3.values.tolist()\ny_train__3 = Y_train__3.values.tolist()\nx_train__4 = X_train__4.values.tolist()\ny_train__4 = Y_train__4.values.tolist()\nx_train__5 = X_train__5.values.tolist()\ny_train__5 = Y_train__5.values.tolist()\nx_train__6 = X_train__6.values.tolist()\ny_train__6 = Y_train__6.values.tolist()\nx_train__7 = X_train__7.values.tolist()\ny_train__7 = Y_train__7.values.tolist()\nx_train__8 = X_train__8.values.tolist()\ny_train__8 = Y_train__8.values.tolist()\nx_train__9 = X_train__9.values.tolist()\ny_train__9 = Y_train__9.values.tolist()\nx_train__10 = X_train__10.values.tolist()\ny_train__10 = Y_train__10.values.tolist()\n\nx_train__1 = scaler.fit_transform(x_train__1)\nx_train__2 = scaler.fit_transform(x_train__2)\nx_train__3 = scaler.fit_transform(x_train__3)\nx_train__4 = scaler.fit_transform(x_train__4)\nx_train__5 = scaler.fit_transform(x_train__5)\nx_train__6 = scaler.fit_transform(x_train__6)\nx_train__7 = scaler.fit_transform(x_train__7)\nx_train__8 = scaler.fit_transform(x_train__8)\nx_train__9 = scaler.fit_transform(x_train__9)\nx_train__10 = scaler.fit_transform(x_train__10)\n\nx_test = X_test.values.tolist()\ny_test = Y_test.values.tolist()\nx_test = scaler.fit_transform(x_test)\n\n\n\nfor i in range(len(y_train__1)) :\n    if y_train__1[i] == 0 :\n        y_train__1[i] = -1\nfor i in range(len(y_train__2)) :\n    if y_train__2[i] == 0 :\n        y_train__2[i] = -1\nfor i in range(len(y_train__3)) :\n    if y_train__3[i] == 0 :\n        y_train__3[i] = -1\nfor i in range(len(y_train__4)) :\n    if y_train__4[i] == 0 :\n        y_train__4[i] = -1\nfor i in range(len(y_train__5)) :\n    if y_train__5[i] == 0 :\n        y_train__5[i] = -1\nfor i in range(len(y_train__6)) :\n    if y_train__6[i] == 0 :\n        y_train__6[i] = -1\nfor i in range(len(y_train__7)) :\n    if y_train__7[i] == 0 :\n        y_train__7[i] = -1\nfor i in range(len(y_train__8)) :\n    if y_train__8[i] == 0 :\n        y_train__8[i] = -1\nfor i in range(len(y_train__9)) :\n    if y_train__9[i] == 0 :\n        y_train__9[i] = -1\nfor i in range(len(y_train__10)) :\n    if y_train__10[i] == 0 :\n        y_train__10[i] = -1\nfor i in range(len(y_test)) :\n    if y_test[i] == 0 :\n        y_test[i] = -1\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T19:38:25.615225Z","iopub.execute_input":"2023-11-27T19:38:25.615603Z","iopub.status.idle":"2023-11-27T19:38:27.597822Z","shell.execute_reply.started":"2023-11-27T19:38:25.615575Z","shell.execute_reply":"2023-11-27T19:38:27.596729Z"},"trusted":true},"execution_count":207,"outputs":[{"name":"stdout","text":"Number of Fraud Transaction = 223\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport random\n\nC = 0.1\ndim = len(x_train__1[0])\nprint(dim)\nw1 = torch.autograd.Variable(torch.rand(dim), requires_grad=True)\nb1 = torch.autograd.Variable(torch.rand(1),   requires_grad=True)\n\nstep_size = 1e-6\nnum_epochs = 500\nminibatch_size = 20\nfor epoch in range(num_epochs):\n    inds = [i for i in range(len(x_train__1))]\n    random.shuffle(inds)\n    for i in range(len(inds)):\n        L = C * (max(0, 1 - y_train__1[inds[i]] * (torch.dot(w1, torch.Tensor(x_train__1[inds[i]])) - b1))**2)\n        if L != 0: # if the loss is zero, Pytorch leaves the variables as a float 0.0, so we can't call backward() on it\n            L.backward()\n            w1.data -= step_size * w1.grad.data # step\n            b1.data -= step_size * b1.grad.data # step\n            w1.grad.data.zero_()\n            b1.grad.data.zero_()\nprint(w1,b1)\n\ndim = len(x_train__2[0])\nprint(dim)\nw2 = torch.autograd.Variable(torch.rand(dim), requires_grad=True)\nb2 = torch.autograd.Variable(torch.rand(1),   requires_grad=True)\n\nstep_size = 1e-6\nnum_epochs = 500\nminibatch_size = 20\nfor epoch in range(num_epochs):\n    inds = [i for i in range(len(x_train__2))]\n    random.shuffle(inds)\n    for i in range(len(inds)):\n        L = C * (max(0, 1 - y_train__2[inds[i]] * (torch.dot(w2, torch.Tensor(x_train__2[inds[i]])) - b2))**2)\n        if L != 0: # if the loss is zero, Pytorch leaves the variables as a float 0.0, so we can't call backward() on it\n            L.backward()\n            w2.data -= step_size * w2.grad.data # step\n            b2.data -= step_size * b2.grad.data # step\n            w2.grad.data.zero_()\n            b2.grad.data.zero_()\nprint(w2,b2)\n\n\n\ndim = len(x_train__3[0])\nprint(dim)\nw3 = torch.autograd.Variable(torch.rand(dim), requires_grad=True)\nb3 = torch.autograd.Variable(torch.rand(1),   requires_grad=True)\n\nstep_size = 1e-6\nnum_epochs = 500\nminibatch_size = 20\nfor epoch in range(num_epochs):\n    inds = [i for i in range(len(x_train__3))]\n    random.shuffle(inds)\n    for i in range(len(inds)):\n        L = C * (max(0, 1 - y_train__3[inds[i]] * (torch.dot(w3, torch.Tensor(x_train__3[inds[i]])) - b3))**2)\n        if L != 0: # if the loss is zero, Pytorch leaves the variables as a float 0.0, so we can't call backward() on it\n            L.backward()\n            w3.data -= step_size * w3.grad.data # step\n            b3.data -= step_size * b3.grad.data # step\n            w3.grad.data.zero_()\n            b3.grad.data.zero_()\nprint(w3,b3)\n\n\ndim = len(x_train__4[0])\nprint(dim)\nw4 = torch.autograd.Variable(torch.rand(dim), requires_grad=True)\nb4 = torch.autograd.Variable(torch.rand(1),   requires_grad=True)\n\nstep_size = 1e-6\nnum_epochs = 500\nminibatch_size = 20\nfor epoch in range(num_epochs):\n    inds = [i for i in range(len(x_train__4))]\n    random.shuffle(inds)\n    for i in range(len(inds)):\n        L = C * (max(0, 1 - y_train__4[inds[i]] * (torch.dot(w4, torch.Tensor(x_train__4[inds[i]])) - b4))**2)\n        if L != 0: # if the loss is zero, Pytorch leaves the variables as a float 0.0, so we can't call backward() on it\n            L.backward()\n            w4.data -= step_size * w4.grad.data # step\n            b4.data -= step_size * b4.grad.data # step\n            w4.grad.data.zero_()\n            b4.grad.data.zero_()\nprint(w4,b4)\n\n\ndim = len(x_train__5[0])\nprint(dim)\nw5 = torch.autograd.Variable(torch.rand(dim), requires_grad=True)\nb5 = torch.autograd.Variable(torch.rand(1),   requires_grad=True)\n\nstep_size = 1e-6\nnum_epochs = 500\nminibatch_size = 20\nfor epoch in range(num_epochs):\n    inds = [i for i in range(len(x_train__5))]\n    random.shuffle(inds)\n    for i in range(len(inds)):\n        L = C * (max(0, 1 - y_train__5[inds[i]] * (torch.dot(w5, torch.Tensor(x_train__5[inds[i]])) - b5))**2)\n        if L != 0: # if the loss is zero, Pytorch leaves the variables as a float 0.0, so we can't call backward() on it\n            L.backward()\n            w5.data -= step_size * w5.grad.data # step\n            b5.data -= step_size * b5.grad.data # step\n            w5.grad.data.zero_()\n            b5.grad.data.zero_()\nprint(w5,b5)\n\n\ndim = len(x_train__6[0])\nprint(dim)\nw6 = torch.autograd.Variable(torch.rand(dim), requires_grad=True)\nb6 = torch.autograd.Variable(torch.rand(1),   requires_grad=True)\n\nstep_size = 1e-6\nnum_epochs = 500\nminibatch_size = 20\nfor epoch in range(num_epochs):\n    inds = [i for i in range(len(x_train__6))]\n    random.shuffle(inds)\n    for i in range(len(inds)):\n        L = C * (max(0, 1 - y_train__6[inds[i]] * (torch.dot(w6, torch.Tensor(x_train__6[inds[i]])) - b6))**2)\n        if L != 0: # if the loss is zero, Pytorch leaves the variables as a float 0.0, so we can't call backward() on it\n            L.backward()\n            w6.data -= step_size * w6.grad.data # step\n            b6.data -= step_size * b6.grad.data # step\n            w6.grad.data.zero_()\n            b6.grad.data.zero_()\nprint(w6,b6)\n\n\ndim = len(x_train__7[0])\nprint(dim)\nw7 = torch.autograd.Variable(torch.rand(dim), requires_grad=True)\nb7 = torch.autograd.Variable(torch.rand(1),   requires_grad=True)\n\nstep_size = 1e-6\nnum_epochs = 500\nminibatch_size = 20\nfor epoch in range(num_epochs):\n    inds = [i for i in range(len(x_train__7))]\n    random.shuffle(inds)\n    for i in range(len(inds)):\n        L = C * (max(0, 1 - y_train__7[inds[i]] * (torch.dot(w7, torch.Tensor(x_train__7[inds[i]])) - b7))**2)\n        if L != 0: # if the loss is zero, Pytorch leaves the variables as a float 0.0, so we can't call backward() on it\n            L.backward()\n            w7.data -= step_size * w7.grad.data # step\n            b7.data -= step_size * b7.grad.data # step\n            w7.grad.data.zero_()\n            b7.grad.data.zero_()\nprint(w7,b7)\n\n\ndim = len(x_train__8[0])\nprint(dim)\nw8 = torch.autograd.Variable(torch.rand(dim), requires_grad=True)\nb8 = torch.autograd.Variable(torch.rand(1),   requires_grad=True)\n\nstep_size = 1e-6\nnum_epochs = 500\nminibatch_size = 20\nfor epoch in range(num_epochs):\n    inds = [i for i in range(len(x_train__8))]\n    random.shuffle(inds)\n    for i in range(len(inds)):\n        L = C * (max(0, 1 - y_train__8[inds[i]] * (torch.dot(w8, torch.Tensor(x_train__8[inds[i]])) - b8))**2)\n        if L != 0: # if the loss is zero, Pytorch leaves the variables as a float 0.0, so we can't call backward() on it\n            L.backward()\n            w8.data -= step_size * w8.grad.data # step\n            b8.data -= step_size * b8.grad.data # step\n            w8.grad.data.zero_()\n            b8.grad.data.zero_()\nprint(w8,b8)\n\n\ndim = len(x_train__9[0])\nprint(dim)\nw9 = torch.autograd.Variable(torch.rand(dim), requires_grad=True)\nb9 = torch.autograd.Variable(torch.rand(1),   requires_grad=True)\n\nstep_size = 1e-6\nnum_epochs = 500\nminibatch_size = 20\nfor epoch in range(num_epochs):\n    inds = [i for i in range(len(x_train__9))]\n    random.shuffle(inds)\n    for i in range(len(inds)):\n        L = C * (max(0, 1 - y_train__9[inds[i]] * (torch.dot(w9, torch.Tensor(x_train__9[inds[i]])) - b9))**2)\n        if L != 0: # if the loss is zero, Pytorch leaves the variables as a float 0.0, so we can't call backward() on it\n            L.backward()\n            w9.data -= step_size * w9.grad.data # step\n            b9.data -= step_size * b9.grad.data # step\n            w9.grad.data.zero_()\n            b9.grad.data.zero_()\nprint(w9,b9)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T20:46:20.246878Z","iopub.execute_input":"2023-11-27T20:46:20.247311Z","iopub.status.idle":"2023-11-27T20:55:16.957434Z","shell.execute_reply.started":"2023-11-27T20:46:20.247279Z","shell.execute_reply":"2023-11-27T20:55:16.956058Z"},"trusted":true},"execution_count":220,"outputs":[{"name":"stdout","text":"29\ntensor([ 0.0982,  0.5177,  0.4151,  0.5229,  0.1092,  0.2851,  0.1016,  1.0259,\n         0.4634, -0.0625,  0.2621,  0.1444,  0.1291,  0.3838,  0.3574,  0.2901,\n        -0.0452,  0.2437,  0.4487,  0.0543,  0.1763,  0.7370,  0.5197,  0.0524,\n         0.4721,  0.6616,  0.9374,  0.1912,  0.3834], requires_grad=True) tensor([0.4467], requires_grad=True)\n29\ntensor([ 0.5934,  0.4972,  0.3061,  0.4216,  0.0470,  0.4822,  0.2690,  0.2725,\n        -0.1431,  0.4909,  0.3540, -0.1134,  0.3922,  0.6170,  0.3373,  0.4386,\n         0.0634, -0.1768,  0.2860,  0.2475,  0.6611,  0.6897,  0.4765,  0.0454,\n         0.8471,  0.4324,  0.9845,  0.3309,  0.6906], requires_grad=True) tensor([0.3073], requires_grad=True)\n29\ntensor([ 0.3610,  0.8172,  0.1382,  0.7256, -0.0893,  0.5799,  0.0748,  0.4162,\n         0.3397,  0.6679,  0.7512, -0.0139,  0.6634,  0.6044,  0.3401,  0.7175,\n        -0.1508,  0.5561,  0.8890,  0.1139,  0.8930,  0.5154,  0.5048,  0.4396,\n         0.5602,  0.2568,  0.2930,  0.4961,  0.7596], requires_grad=True) tensor([0.0451], requires_grad=True)\n29\ntensor([ 0.2561,  0.9890,  0.0839,  0.5401,  0.4215,  0.1932,  0.2812,  0.6687,\n         0.6512,  0.4884,  1.0845,  0.6731,  0.4033,  0.5028,  0.0660, -0.2073,\n         0.2188,  0.6491,  1.0969,  0.6748,  0.2902,  0.6698,  0.7638,  0.4564,\n         0.6945,  0.4324,  0.1148,  0.0406,  0.1338], requires_grad=True) tensor([0.8816], requires_grad=True)\n29\ntensor([ 0.5411,  0.4686,  0.5329,  1.0434,  0.1873,  0.2004, -0.2875,  0.6009,\n         0.4010,  0.6421,  0.5690,  0.3967,  0.1602,  0.6191,  0.0648,  0.5007,\n        -0.0432, -0.0575,  0.3483,  0.4235,  0.3045,  0.3104,  0.7856,  0.2159,\n         0.0294,  0.1989,  1.0230,  0.0920,  0.0612], requires_grad=True) tensor([0.6361], requires_grad=True)\n29\ntensor([-0.0761,  0.5962,  0.6257,  0.4134,  0.8582,  0.1685,  0.0419,  0.4780,\n         0.1180,  0.4364,  0.9654, -0.0177,  0.4159, -0.0586,  0.1972,  0.0418,\n        -0.0633,  0.2692,  0.2286,  0.4072,  0.9885,  0.5408,  0.5513,  0.7113,\n         0.0805,  0.0163,  0.9482,  0.0927,  0.7446], requires_grad=True) tensor([0.7612], requires_grad=True)\n29\ntensor([ 0.0981,  1.1070,  0.5454,  0.9459,  0.1705,  0.3994,  0.6319,  0.7191,\n        -0.0895,  0.5777,  0.8156,  0.2039,  0.6249,  0.3269,  0.3132,  0.4189,\n         0.5262,  0.3798,  0.3621,  0.6305,  0.9304,  0.6711,  0.6346,  0.0594,\n         0.0620,  0.5865,  0.5058,  0.4005,  0.8243], requires_grad=True) tensor([0.1854], requires_grad=True)\n29\ntensor([ 0.5333,  0.8245,  0.4130,  0.3419,  0.4030,  0.1850,  0.0767,  0.2961,\n        -0.2023,  0.6951,  1.0923,  0.2262,  0.8894,  0.1377,  0.3096,  0.2423,\n         0.7127, -0.0586,  0.3053,  0.1276,  0.1232,  0.1381, -0.0030,  0.3967,\n         0.5542,  0.4436,  0.9345,  0.2758,  0.7363], requires_grad=True) tensor([0.1087], requires_grad=True)\n29\ntensor([ 0.5392,  1.1643, -0.1713,  1.0083,  0.6850,  0.2123, -0.1040,  0.9844,\n         0.6465,  0.2174,  0.3010,  0.6777,  0.5495,  0.5286,  0.3204,  0.3901,\n         0.6053,  0.1351,  0.2509,  0.1755,  0.9740,  0.2703,  0.2944,  0.1463,\n         0.1579,  0.7141,  0.7470,  0.2620,  0.4818], requires_grad=True) tensor([0.4374], requires_grad=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\n\nprint('plane equation:  w=', w.detach().numpy(), 'b =', b.detach().numpy()[0])\ndef accuracy(X, y):\n    correct = 0\n    for i in range(len(y)):\n        y_predicted = int(np.sign((torch.dot(w1, torch.Tensor(X[i])) - b1).detach().numpy()[0]))\n        if y_predicted == y[i]: correct += 1\n    return float(correct)/len(y)\n\nprint('train accuracy', accuracy(x_train__1, y_train__1))\nprint('test accuracy', accuracy(x_test, y_test))\n\n\n\nprint('plane equation:  w=', w.detach().numpy(), 'b =', b.detach().numpy()[0])\ndef accuracy(X, y):\n    correct = 0\n    for i in range(len(y)):\n        y_predicted = int(np.sign((torch.dot(w2, torch.Tensor(X[i])) - b2).detach().numpy()[0]))\n        if y_predicted == y[i]: correct += 1\n    return float(correct)/len(y)\n\nprint('train accuracy', accuracy(x_train__2, y_train__2))\nprint('test accuracy', accuracy(x_test, y_test))\n\n\n\nprint('plane equation:  w=', w.detach().numpy(), 'b =', b.detach().numpy()[0])\ndef accuracy(X, y):\n    correct = 0\n    for i in range(len(y)):\n        y_predicted = int(np.sign((torch.dot(w3, torch.Tensor(X[i])) - b3).detach().numpy()[0]))\n        if y_predicted == y[i]: correct += 1\n    return float(correct)/len(y)\n\nprint('train accuracy', accuracy(x_train__3, y_train__3))\nprint('test accuracy', accuracy(x_test, y_test))\n\n\nprint('plane equation:  w=', w.detach().numpy(), 'b =', b.detach().numpy()[0])\ndef accuracy(X, y):\n    correct = 0\n    for i in range(len(y)):\n        y_predicted = int(np.sign((torch.dot(w4, torch.Tensor(X[i])) - b4).detach().numpy()[0]))\n        if y_predicted == y[i]: correct += 1\n    return float(correct)/len(y)\n\nprint('train accuracy', accuracy(x_train__4, y_train__4))\nprint('test accuracy', accuracy(x_test, y_test))\n\n\nprint('plane equation:  w=', w.detach().numpy(), 'b =', b.detach().numpy()[0])\ndef accuracy(X, y):\n    correct = 0\n    for i in range(len(y)):\n        y_predicted = int(np.sign((torch.dot(w5, torch.Tensor(X[i])) - b5).detach().numpy()[0]))\n        if y_predicted == y[i]: correct += 1\n    return float(correct)/len(y)\n\nprint('train accuracy', accuracy(x_train__5, y_train__5))\nprint('test accuracy', accuracy(x_test, y_test))\n\n\nprint('plane equation:  w=', w.detach().numpy(), 'b =', b.detach().numpy()[0])\ndef accuracy(X, y):\n    correct = 0\n    for i in range(len(y)):\n        y_predicted = int(np.sign((torch.dot(w6, torch.Tensor(X[i])) - b6).detach().numpy()[0]))\n        if y_predicted == y[i]: correct += 1\n    return float(correct)/len(y)\n\nprint('train accuracy', accuracy(x_train__6, y_train__6))\nprint('test accuracy', accuracy(x_test, y_test))\n\n\nprint('plane equation:  w=', w.detach().numpy(), 'b =', b.detach().numpy()[0])\ndef accuracy(X, y):\n    correct = 0\n    for i in range(len(y)):\n        y_predicted = int(np.sign((torch.dot(w7, torch.Tensor(X[i])) - b7).detach().numpy()[0]))\n        if y_predicted == y[i]: correct += 1\n    return float(correct)/len(y)\n\nprint('train accuracy', accuracy(x_train__7, y_train__7))\nprint('test accuracy', accuracy(x_test, y_test))\n\n\n\nprint('plane equation:  w=', w.detach().numpy(), 'b =', b.detach().numpy()[0])\ndef accuracy(X, y):\n    correct = 0\n    for i in range(len(y)):\n        y_predicted = int(np.sign((torch.dot(w8, torch.Tensor(X[i])) - b8).detach().numpy()[0]))\n        if y_predicted == y[i]: correct += 1\n    return float(correct)/len(y)\n\nprint('train accuracy', accuracy(x_train__8, y_train__8))\nprint('test accuracy', accuracy(x_test, y_test))\n\n\nprint('plane equation:  w=', w.detach().numpy(), 'b =', b.detach().numpy()[0])\ndef accuracy(X, y):\n    correct = 0\n    for i in range(len(y)):\n        y_predicted = int(np.sign((torch.dot(w9, torch.Tensor(X[i])) - b9).detach().numpy()[0]))\n        if y_predicted == y[i]: correct += 1\n    return float(correct)/len(y)\n\nprint('train accuracy', accuracy(x_train__9, y_train__9))\nprint('test accuracy', accuracy(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-11-27T20:55:28.237915Z","iopub.execute_input":"2023-11-27T20:55:28.238370Z","iopub.status.idle":"2023-11-27T20:56:18.153980Z","shell.execute_reply.started":"2023-11-27T20:55:28.238325Z","shell.execute_reply":"2023-11-27T20:56:18.152678Z"},"trusted":true},"execution_count":221,"outputs":[{"name":"stdout","text":"plane equation:  w= [ 0.05662426  0.22886802 -0.17517106  0.3631412   0.7576581  -0.10271765\n -0.16986188  0.2745875   0.1871479   0.48516273  0.29159537  0.23220786\n  0.09014101  0.67044294  0.88776237  0.4560186   0.6516656   0.18677628\n  0.9792622   0.31844077  0.14111124  0.36665717  0.04553364  0.92214006\n  0.92088187  0.83406633  0.2608149   0.08971967  0.1773639 ] b = 0.56405\ntrain accuracy 0.6278026905829597\ntest accuracy 0.5791122630636285\nplane equation:  w= [ 0.05662426  0.22886802 -0.17517106  0.3631412   0.7576581  -0.10271765\n -0.16986188  0.2745875   0.1871479   0.48516273  0.29159537  0.23220786\n  0.09014101  0.67044294  0.88776237  0.4560186   0.6516656   0.18677628\n  0.9792622   0.31844077  0.14111124  0.36665717  0.04553364  0.92214006\n  0.92088187  0.83406633  0.2608149   0.08971967  0.1773639 ] b = 0.56405\ntrain accuracy 0.5037369207772795\ntest accuracy 0.5495841607731309\nplane equation:  w= [ 0.05662426  0.22886802 -0.17517106  0.3631412   0.7576581  -0.10271765\n -0.16986188  0.2745875   0.1871479   0.48516273  0.29159537  0.23220786\n  0.09014101  0.67044294  0.88776237  0.4560186   0.6516656   0.18677628\n  0.9792622   0.31844077  0.14111124  0.36665717  0.04553364  0.92214006\n  0.92088187  0.83406633  0.2608149   0.08971967  0.1773639 ] b = 0.56405\ntrain accuracy 0.5515695067264574\ntest accuracy 0.5108356285205647\nplane equation:  w= [ 0.05662426  0.22886802 -0.17517106  0.3631412   0.7576581  -0.10271765\n -0.16986188  0.2745875   0.1871479   0.48516273  0.29159537  0.23220786\n  0.09014101  0.67044294  0.88776237  0.4560186   0.6516656   0.18677628\n  0.9792622   0.31844077  0.14111124  0.36665717  0.04553364  0.92214006\n  0.92088187  0.83406633  0.2608149   0.08971967  0.1773639 ] b = 0.56405\ntrain accuracy 0.5605381165919282\ntest accuracy 0.6260855919959741\nplane equation:  w= [ 0.05662426  0.22886802 -0.17517106  0.3631412   0.7576581  -0.10271765\n -0.16986188  0.2745875   0.1871479   0.48516273  0.29159537  0.23220786\n  0.09014101  0.67044294  0.88776237  0.4560186   0.6516656   0.18677628\n  0.9792622   0.31844077  0.14111124  0.36665717  0.04553364  0.92214006\n  0.92088187  0.83406633  0.2608149   0.08971967  0.1773639 ] b = 0.56405\ntrain accuracy 0.5724962630792227\ntest accuracy 0.5997770647215744\nplane equation:  w= [ 0.05662426  0.22886802 -0.17517106  0.3631412   0.7576581  -0.10271765\n -0.16986188  0.2745875   0.1871479   0.48516273  0.29159537  0.23220786\n  0.09014101  0.67044294  0.88776237  0.4560186   0.6516656   0.18677628\n  0.9792622   0.31844077  0.14111124  0.36665717  0.04553364  0.92214006\n  0.92088187  0.83406633  0.2608149   0.08971967  0.1773639 ] b = 0.56405\ntrain accuracy 0.7518684603886397\ntest accuracy 0.6517231490149182\nplane equation:  w= [ 0.05662426  0.22886802 -0.17517106  0.3631412   0.7576581  -0.10271765\n -0.16986188  0.2745875   0.1871479   0.48516273  0.29159537  0.23220786\n  0.09014101  0.67044294  0.88776237  0.4560186   0.6516656   0.18677628\n  0.9792622   0.31844077  0.14111124  0.36665717  0.04553364  0.92214006\n  0.92088187  0.83406633  0.2608149   0.08971967  0.1773639 ] b = 0.56405\ntrain accuracy 0.5964125560538116\ntest accuracy 0.5507313034679422\nplane equation:  w= [ 0.05662426  0.22886802 -0.17517106  0.3631412   0.7576581  -0.10271765\n -0.16986188  0.2745875   0.1871479   0.48516273  0.29159537  0.23220786\n  0.09014101  0.67044294  0.88776237  0.4560186   0.6516656   0.18677628\n  0.9792622   0.31844077  0.14111124  0.36665717  0.04553364  0.92214006\n  0.92088187  0.83406633  0.2608149   0.08971967  0.1773639 ] b = 0.56405\ntrain accuracy 0.5381165919282511\ntest accuracy 0.5126429193699373\nplane equation:  w= [ 0.05662426  0.22886802 -0.17517106  0.3631412   0.7576581  -0.10271765\n -0.16986188  0.2745875   0.1871479   0.48516273  0.29159537  0.23220786\n  0.09014101  0.67044294  0.88776237  0.4560186   0.6516656   0.18677628\n  0.9792622   0.31844077  0.14111124  0.36665717  0.04553364  0.92214006\n  0.92088187  0.83406633  0.2608149   0.08971967  0.1773639 ] b = 0.56405\ntrain accuracy 0.5650224215246636\ntest accuracy 0.5288652486107128\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\n\nprint('plane equation:  w=', w.detach().numpy(), 'b =', b.detach().numpy()[0])\ndef accuracy(X, y):\n    correct = 0\n    for i in range(len(y)):\n        count = 0\n        y_predicted_1 = int(np.sign((torch.dot(w1, torch.Tensor(X[i])) - b1).detach().numpy()[0]))\n        y_predicted_2 = int(np.sign((torch.dot(w2, torch.Tensor(X[i])) - b2).detach().numpy()[0]))\n        y_predicted_3 = int(np.sign((torch.dot(w3, torch.Tensor(X[i])) - b3).detach().numpy()[0]))\n        y_predicted_4 = int(np.sign((torch.dot(w4, torch.Tensor(X[i])) - b4).detach().numpy()[0]))\n        y_predicted_5 = int(np.sign((torch.dot(w5, torch.Tensor(X[i])) - b5).detach().numpy()[0]))\n        y_predicted_6 = int(np.sign((torch.dot(w6, torch.Tensor(X[i])) - b6).detach().numpy()[0]))\n        y_predicted_7 = int(np.sign((torch.dot(w6, torch.Tensor(X[i])) - b6).detach().numpy()[0]))\n        y_predicted_8 = int(np.sign((torch.dot(w8, torch.Tensor(X[i])) - b8).detach().numpy()[0]))\n        y_predicted_9 = int(np.sign((torch.dot(w9, torch.Tensor(X[i])) - b9).detach().numpy()[0]))\n        if y_predicted_1 == y[i]: \n            count += 1\n        if y_predicted_2 == y[i]: \n            count += 1\n        if y_predicted_3 == y[i]: \n            count += 1\n        if y_predicted_4 == y[i]: \n            count += 1\n        if y_predicted_5 == y[i]: \n            count += 1\n        if y_predicted_6 == y[i]: \n            count += 1\n        if y_predicted_7 == y[i]: \n            count += 1\n        if y_predicted_8 == y[i]: \n            count += 1\n        if y_predicted_9 == y[i]: \n            count += 1\n        if count > 4 :\n            correct += 1\n    return float(correct)/len(y)\n\nprint('train accuracy', accuracy(x_train__1, y_train__1))\nprint('train accuracy', accuracy(x_train__2, y_train__2))\nprint('train accuracy', accuracy(x_train__3, y_train__3))\nprint('train accuracy', accuracy(x_train__4, y_train__4))\nprint('train accuracy', accuracy(x_train__5, y_train__5))\nprint('train accuracy', accuracy(x_train__6, y_train__6))\nprint('train accuracy', accuracy(x_train__7, y_train__7))\nprint('train accuracy', accuracy(x_train__8, y_train__8))\nprint('train accuracy', accuracy(x_train__9, y_train__9))\nprint('test accuracy', accuracy(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-11-27T20:56:34.043466Z","iopub.execute_input":"2023-11-27T20:56:34.043840Z","iopub.status.idle":"2023-11-27T20:57:24.965546Z","shell.execute_reply.started":"2023-11-27T20:56:34.043811Z","shell.execute_reply":"2023-11-27T20:57:24.964101Z"},"trusted":true},"execution_count":222,"outputs":[{"name":"stdout","text":"plane equation:  w= [ 0.05662426  0.22886802 -0.17517106  0.3631412   0.7576581  -0.10271765\n -0.16986188  0.2745875   0.1871479   0.48516273  0.29159537  0.23220786\n  0.09014101  0.67044294  0.88776237  0.4560186   0.6516656   0.18677628\n  0.9792622   0.31844077  0.14111124  0.36665717  0.04553364  0.92214006\n  0.92088187  0.83406633  0.2608149   0.08971967  0.1773639 ] b = 0.56405\ntrain accuracy 0.617339312406577\ntrain accuracy 0.6158445440956651\ntrain accuracy 0.6188340807174888\ntrain accuracy 0.6083707025411061\ntrain accuracy 0.5979073243647235\ntrain accuracy 0.5964125560538116\ntrain accuracy 0.6098654708520179\ntrain accuracy 0.6233183856502242\ntrain accuracy 0.6188340807174888\ntest accuracy 0.593153938974173\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**The accuracy of the ensembled models was equivalent to the accuracy of single classifier. Ensembling methods didn't seem to improve the performance of our Linear SVM model and the accuracy on the test data-set was still 60%**","metadata":{}}]}